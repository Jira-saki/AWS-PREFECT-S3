# GitHub Trending Repos Data Pipeline

A production-ready data engineering pipeline that tracks trending GitHub repositories using Prefect orchestration and AWS services.

## ğŸ¯ Project Overview

This pipeline demonstrates:
- **API Integration**: Fetch trending repositories from GitHub API
- **Data Transformation**: Convert nested JSON to structured Parquet format
- **Cloud Storage**: Store raw and processed data in AWS S3 with date partitioning
- **Workflow Orchestration**: Automated pipeline using Prefect
- **Scalable Architecture**: Ready for deployment on AWS ECS Fargate

## ğŸ—ï¸ Architecture

```
GitHub API
    â†“
Prefect Flow (Orchestration)
    â†“
â”œâ”€â”€ Extract: Fetch trending repos
â”œâ”€â”€ Load: Save raw JSON to S3
â”œâ”€â”€ Transform: Flatten and clean data
â””â”€â”€ Load: Save Parquet to S3
```

**Data Flow:**
```
s3://bucket/
â”œâ”€â”€ raw/year=YYYY/month=MM/day=DD/*.json
â””â”€â”€ processed/year=YYYY/month=MM/day=DD/*.parquet
```

## ğŸš€ Features

- âœ… Automated data ingestion from GitHub API
- âœ… Date-based partitioning for efficient querying
- âœ… Data quality checks and cleaning
- âœ… Retry logic and error handling
- âœ… Parquet format for optimized storage
- âœ… Ready for production deployment

## ï¿½  Screenshots

### Pipeline Execution
![Pipeline Run](images/prefect-pipeline-complete.png)

### S3 Bucket Structure
![S3 Structure](images/s3-bucket.png)

### GitHub Actions CI/CD
![GitHub Actions](images/gitaction.png)

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_client.py      # GitHub API client
â”‚   â”œâ”€â”€ storage.py         # S3 storage operations
â”‚   â”œâ”€â”€ transform.py       # Data transformation
â”‚   â””â”€â”€ config.py          # Configuration management
â”œâ”€â”€ flows/
â”‚   â””â”€â”€ github_pipeline.py # Prefect flow definition
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py          # Unit tests
â”œâ”€â”€ .env.example           # Environment variables template
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

## ğŸ› ï¸ Setup

### Prerequisites

- Python 3.11+
- AWS Account with S3 access
- GitHub Personal Access Token
- (Optional) Prefect Cloud account

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/github-data-pipeline.git
cd github-data-pipeline
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your credentials
```

5. **Set up AWS credentials**
- Create S3 bucket
- Configure IAM user with S3 access
- Add credentials to `.env`

6. **Get GitHub token**
- Go to https://github.com/settings/tokens
- Generate token with `public_repo` scope
- Add to `.env`

## ğŸƒ Usage

### Run Pipeline Locally

```bash
python flows/github_pipeline.py
```

### Run with Custom Parameters

```python
from flows.github_pipeline import github_pipeline

github_pipeline(
    language="python",  # or "javascript", "go", etc.
    days_back=7        # repos created in last 7 days
)
```

## ğŸ“Š Data Schema

### Raw Data (JSON)
Stored in: `s3://bucket/raw/year=YYYY/month=MM/day=DD/`

### Processed Data (Parquet)
Stored in: `s3://bucket/processed/year=YYYY/month=MM/day=DD/`

**Columns:**
- `repo_id`: Repository ID
- `repo_name`: Repository name
- `full_name`: Owner/repo name
- `owner_login`: Owner username
- `stars`: Star count
- `forks`: Fork count
- `language`: Primary language
- `description`: Repository description
- `created_at`: Creation timestamp
- `updated_at`: Last update timestamp
- `extracted_at`: Data extraction timestamp
- `url`: Repository URL

## ğŸš¢ Deployment Options

### Option 1: AWS EC2
- Simple deployment
- Fixed monthly cost
- Good for learning

### Option 2: AWS ECS Fargate (Recommended)
- Serverless containers
- Pay per use
- Auto-scaling
- Production-ready

### Option 3: Prefect Cloud + AWS
- Managed orchestration
- Easy monitoring
- Scheduling UI

## ğŸ§ª Testing

```bash
# Test API client
python test_connection.py

# Test S3 storage
python test_s3.py

# Test transformation
jupyter notebook transform_practice.ipynb
```

## ğŸ“ˆ Future Enhancements

- [ ] Add Terraform for infrastructure as code
- [ ] Implement data quality tests
- [ ] Add multiple language support
- [ ] Create Athena tables for querying
- [ ] Add email/Slack notifications
- [ ] Implement incremental loading
- [ ] Add data visualization dashboard

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

- GitHub: [@Jira-saki](https://github.com/Jira-saki)
- LinkedIn: [LinkedIn](https://linkedin.com/in/jira-saki)

## ğŸ™ Acknowledgments

- GitHub API for providing data
- Prefect for workflow orchestration
- AWS for cloud infrastructure